{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ratings: 100836\n",
      "Users: 610\n",
      "Movies: 9724\n",
      "Rating distribution:\n",
      "0.5     1370\n",
      "1.0     2811\n",
      "1.5     1791\n",
      "2.0     7551\n",
      "2.5     5550\n",
      "3.0    20047\n",
      "3.5    13136\n",
      "4.0    26818\n",
      "4.5     8551\n",
      "5.0    13211\n",
      "Name: rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('Data/ratings.csv')\n",
    "movies = pd.read_csv('Data/movies.csv')\n",
    "\n",
    "ratings.drop('timestamp', axis=1, inplace=True)\n",
    "merged_data = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "print(f\"Total ratings: {len(ratings)}\")\n",
    "print(f\"Users: {ratings['userId'].nunique()}\")\n",
    "print(f\"Movies: {ratings['movieId'].nunique()}\")\n",
    "print(f\"Rating distribution:\\n{ratings['rating'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 80668 ratings\n",
      "Test data: 20168 ratings\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42, stratify=ratings['userId'])\n",
    "\n",
    "print(f\"Train data: {len(train_data)} ratings\")\n",
    "print(f\"Test data: {len(test_data)} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-item matrix shape: (610, 8977)\n"
     ]
    }
   ],
   "source": [
    "# Create user-item matrix from training data only\n",
    "user_item_matrix = train_data.pivot_table(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "print(f\"User-item matrix shape: {user_item_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "\n",
    "def get_user_based_recommendations(user_id, k=10):\n",
    "    if user_id not in user_similarity_df.index:\n",
    "        return []\n",
    "    \n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:k+1]\n",
    "    \n",
    "    if len(similar_users) == 0 or similar_users.sum() == 0:\n",
    "        return []\n",
    "    \n",
    "    weighted_ratings = user_item_matrix.loc[similar_users.index].T.dot(similar_users)\n",
    "    normalized_ratings = weighted_ratings / similar_users.sum()\n",
    "    user_seen = user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index\n",
    "    recommendations = normalized_ratings.drop(user_seen, errors='ignore').sort_values(ascending=False).head(k)\n",
    "    return recommendations.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity = cosine_similarity(user_item_matrix.T)\n",
    "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
    "\n",
    "def get_item_based_recommendations(user_id, k=10):\n",
    "    if user_id not in user_item_matrix.index:\n",
    "        return []\n",
    "        \n",
    "    user_ratings = user_item_matrix.loc[user_id]\n",
    "    user_rated_items = user_ratings[user_ratings > 0]\n",
    "    \n",
    "    if len(user_rated_items) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Calculate scores based on item similarity\n",
    "    scores = pd.Series(0.0, index=user_item_matrix.columns)\n",
    "    \n",
    "    for item_id, rating in user_rated_items.items():\n",
    "        if item_id in item_similarity_df.columns:\n",
    "            similar_items = item_similarity_df[item_id]\n",
    "            scores += similar_items * rating\n",
    "    \n",
    "    # Normalize scores\n",
    "    scores = scores / len(user_rated_items)\n",
    "    \n",
    "    # Remove already seen items\n",
    "    user_seen = user_rated_items.index\n",
    "    recommendations = scores.drop(user_seen, errors='ignore').sort_values(ascending=False).head(k)\n",
    "    return recommendations.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD-based collaborative filtering\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "latent_matrix = svd.fit_transform(user_item_matrix)\n",
    "pred_matrix = np.dot(latent_matrix, svd.components_)\n",
    "pred_df = pd.DataFrame(pred_matrix, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "\n",
    "def get_svd_recommendations(user_id, k=10):\n",
    "    if user_id not in pred_df.index:\n",
    "        return []\n",
    "        \n",
    "    user_predictions = pred_df.loc[user_id]\n",
    "    user_seen = user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index\n",
    "    recommendations = user_predictions.drop(user_seen, errors='ignore').sort_values(ascending=False).head(k)\n",
    "    return recommendations.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(user_id, recommendation_func, test_data, k=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Calculate precision@k for a user using proper train/test split\n",
    "    \"\"\"\n",
    "    user_test_ratings = test_data[test_data['userId'] == user_id]\n",
    "    \n",
    "    if len(user_test_ratings) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    relevant_items = set(user_test_ratings[user_test_ratings['rating'] >= threshold]['movieId'].tolist())\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        # Get recommendations\n",
    "        recommendations = recommendation_func(user_id, k)\n",
    "        \n",
    "        if not recommendations:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate hits\n",
    "        recommended_set = set(recommendations)\n",
    "        hits = len(recommended_set.intersection(relevant_items))\n",
    "        \n",
    "        return hits / min(k, len(recommendations))\n",
    "    \n",
    "    except Exception as e:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_methods(test_data, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate all recommendation methods\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        'User-based CF': get_user_based_recommendations,\n",
    "        'Item-based CF': get_item_based_recommendations,\n",
    "        'SVD CF': get_svd_recommendations\n",
    "    }\n",
    "    \n",
    "    # Get users that exist in both train and test sets\n",
    "    test_users = set(test_data['userId'].unique())\n",
    "    train_users = set(user_item_matrix.index)\n",
    "    common_users = list(test_users.intersection(train_users))\n",
    "    \n",
    "    print(f\"Evaluating on {len(common_users)} users who appear in both train and test sets\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method_name, method_func in methods.items():\n",
    "        print(f\"\\nEvaluating {method_name}...\")\n",
    "        \n",
    "        precisions = []\n",
    "        \n",
    "        for user_id in common_users:\n",
    "            precision = precision_at_k(user_id, method_func, test_data, k=k)\n",
    "            if not np.isnan(precision):\n",
    "                precisions.append(precision)\n",
    "        \n",
    "        if len(precisions) > 0:\n",
    "            mean_precision = np.mean(precisions)\n",
    "            std_precision = np.std(precisions)\n",
    "            results[method_name] = {\n",
    "                'mean_precision': mean_precision,\n",
    "                'std_precision': std_precision,\n",
    "                'num_users_evaluated': len(precisions)\n",
    "            }\n",
    "            print(f\"{method_name} - Mean Precision@{k}: {mean_precision:.4f}\")\n",
    "            print(f\"Evaluated on {len(precisions)} users\")\n",
    "            \n",
    "            # Show some examples of non-zero precisions\n",
    "            non_zero_precisions = [p for p in precisions if p > 0]\n",
    "            if non_zero_precisions:\n",
    "                print(f\"Users with non-zero precision: {len(non_zero_precisions)} ({len(non_zero_precisions)/len(precisions)*100:.1f}%)\")\n",
    "                print(f\"Mean precision for users with hits: {np.mean(non_zero_precisions):.4f}\")\n",
    "        else:\n",
    "            results[method_name] = {\n",
    "                'mean_precision': 0.0,\n",
    "                'std_precision': 0.0,\n",
    "                'num_users_evaluated': 0\n",
    "            }\n",
    "            print(f\"{method_name} - No valid evaluations possible\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 610 users who appear in both train and test sets\n",
      "\n",
      "Evaluating User-based CF...\n",
      "User-based CF - Mean Precision@5: 0.1950\n",
      "Evaluated on 599 users\n",
      "Users with non-zero precision: 343 (57.3%)\n",
      "Mean precision for users with hits: 0.3405\n",
      "\n",
      "Evaluating Item-based CF...\n",
      "Item-based CF - Mean Precision@5: 0.1863\n",
      "Evaluated on 599 users\n",
      "Users with non-zero precision: 331 (55.3%)\n",
      "Mean precision for users with hits: 0.3372\n",
      "\n",
      "Evaluating SVD CF...\n",
      "SVD CF - Mean Precision@5: 0.2397\n",
      "Evaluated on 599 users\n",
      "Users with non-zero precision: 357 (59.6%)\n",
      "Mean precision for users with hits: 0.4022\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_methods(test_data, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
